{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d1ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim \n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e2faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a92e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "device=''\n",
    "if torch.cuda.is_available():\n",
    "      device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "#path = \n",
    "os.makedirs(\"images\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfaab53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Linear: 1-1                            [-1, 1, 256]              33,024\n",
      "├─LeakyReLU: 1-2                         [-1, 1, 256]              --\n",
      "├─Linear: 1-3                            [-1, 1, 512]              131,584\n",
      "├─LeakyReLU: 1-4                         [-1, 1, 512]              --\n",
      "├─Linear: 1-5                            [-1, 1, 784]              402,192\n",
      "├─Tanh: 1-6                              [-1, 1, 784]              --\n",
      "==========================================================================================\n",
      "Total params: 566,800\n",
      "Trainable params: 566,800\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.57\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 2.16\n",
      "Estimated Total Size (MB): 2.17\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Linear: 1-1                            [-1, 1, 1024]             803,840\n",
      "├─LeakyReLU: 1-2                         [-1, 1, 1024]             --\n",
      "├─Linear: 1-3                            [-1, 1, 512]              524,800\n",
      "├─LeakyReLU: 1-4                         [-1, 1, 512]              --\n",
      "├─Linear: 1-5                            [-1, 1, 256]              131,328\n",
      "├─LeakyReLU: 1-6                         [-1, 1, 256]              --\n",
      "├─Linear: 1-7                            [-1, 1, 1]                257\n",
      "├─Sigmoid: 1-8                           [-1, 1, 1]                --\n",
      "==========================================================================================\n",
      "Total params: 1,460,225\n",
      "Trainable params: 1,460,225\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 1.46\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 5.57\n",
      "Estimated Total Size (MB): 5.59\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Linear: 1-1                            [-1, 1, 1024]             803,840\n",
       "├─LeakyReLU: 1-2                         [-1, 1, 1024]             --\n",
       "├─Linear: 1-3                            [-1, 1, 512]              524,800\n",
       "├─LeakyReLU: 1-4                         [-1, 1, 512]              --\n",
       "├─Linear: 1-5                            [-1, 1, 256]              131,328\n",
       "├─LeakyReLU: 1-6                         [-1, 1, 256]              --\n",
       "├─Linear: 1-7                            [-1, 1, 1]                257\n",
       "├─Sigmoid: 1-8                           [-1, 1, 1]                --\n",
       "==========================================================================================\n",
       "Total params: 1,460,225\n",
       "Trainable params: 1,460,225\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.46\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 5.57\n",
       "Estimated Total Size (MB): 5.59\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super (Generator,self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.relu1 = nn.LeakyReLU(0.25)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.relu2 = nn.LeakyReLU(0.25)\n",
    "        self.fc3 = nn.Linear(512,784)\n",
    "        self.activation=nn.Tanh()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.relu1(self.fc1(x))\n",
    "        x=self.relu2(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self). __init__()\n",
    "        self.fc1 = nn.Linear(784, 1024)\n",
    "        self.relu1 = nn.LeakyReLU(0.25)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.relu2 = nn.LeakyReLU(0.25)\n",
    "        self.fc3 = nn.Linear(512,256)\n",
    "        self.relu3 = nn.LeakyReLU(0.25)\n",
    "        self.fc4 = nn.Linear(256,1)\n",
    "        self.final= nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        x=self.relu1(self.fc1(x))\n",
    "        x=self.relu2(self.fc2(x))\n",
    "        x=self.relu3(self.fc3(x))    \n",
    "        x=self.fc4(x)\n",
    "        return self.final(x)\n",
    "\n",
    "\n",
    "model= Generator()\n",
    "summary(model,(1,128))\n",
    "\n",
    "\n",
    "model2= Discriminator()\n",
    "summary(model2,(1,784))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae5f9ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = torch.nn.BCELoss().to(device)\n",
    "generator = Generator().to(device)\n",
    "discriminator=Discriminator().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e01f6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/OSU/Thesis/Fairness/MNIST/archive/trainingSet/trainingSet/0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'D:/OSU/Thesis/Fairness/MNIST/archive/trainingSet/trainingSet/'+str('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec2c07c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  path  label\n",
      "0   p1      1\n",
      "1   p2      2\n",
      "2   p3      3\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data={'path':['p1','p2','p3'],'label':[1,2,3]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb5ae2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4132"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('D:/OSU/Thesis/Fairness/MNIST/archive/trainingSample/trainingSample/0'))\n",
    "len(os.listdir('D:/OSU/Thesis/Fairness/MNIST/archive/trainingSet/trainingSet/0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "864ad0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def generate_train_data():\n",
    "    label_dict = dict([(j,i) for (i,j) in list(enumerate(os.listdir('D:/OSU/Thesis/Fairness/MNIST/archive/trainingSet/trainingSet/')))])\n",
    "    print(label_dict)\n",
    "\n",
    "    paths = []\n",
    "    label = []\n",
    "    for i in tqdm(os.listdir('D:/OSU/Thesis/Fairness/MNIST/archive/trainingSet/trainingSet/')):\n",
    "        for j in os.listdir('D:/OSU/Thesis/Fairness/MNIST/archive/trainingSet/trainingSet/'+str(i))[:100]:\n",
    "            paths.append('D:/OSU/Thesis/Fairness/MNIST/archive/trainingSet/trainingSet/'+str(i)+'/'+str(j))\n",
    "            label.append(i)\n",
    "    train_df = pd.DataFrame({\"paths\": paths, \"label\": label})  \n",
    "    train_df = train_df.sample(frac=1)\n",
    "    return train_df\n",
    "\n",
    "def generate_test_data():\n",
    "    paths = []\n",
    "    label = []\n",
    "    for i in tqdm(os.listdir('D:/OSU/Thesis/Fairness/MNIST/archive/trainingSample/trainingSample/')):\n",
    "        for j in os.listdir('D:/OSU/Thesis/Fairness/MNIST/archive/trainingSample/trainingSample/'+str(i))[:50]:\n",
    "            paths.append('D:/OSU/Thesis/Fairness/MNIST/archive/trainingSample/trainingSample/'+str(i)+'/'+str(j))\n",
    "            label.append(i)\n",
    "    test_df = pd.DataFrame({\"paths\": paths, \"label\": label})\n",
    "    return test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d9f17d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                      | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 263.00it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = generate_train_data()\n",
    "test_df = generate_test_data()\n",
    "train_df.to_csv('./csv/train.csv')\n",
    "test_df.to_csv('./csv/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01989592",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFARDataSet(Dataset):\n",
    "    def __init__(self, csv_file, transform,device):\n",
    "        self.transform = transform\n",
    "        self.df = csv_file\n",
    "        self.device = device\n",
    "        #self.standard_deviaton=std\n",
    "        #self.mean2=mean\n",
    "    def __len__(self):\n",
    "        return len(self.df) \n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        label = label_dict[self.df.iloc[idx]['label']]\n",
    "        label_onehot = np.zeros(10)\n",
    "        label_onehot[label] = 1.\n",
    "        path = self.df.iloc[idx]['paths']\n",
    "        image = Image.open(path)\n",
    "        image = self.transform(image)\n",
    "        arr = np.random.randint(low = 0, high = 255, size = (300, 300, 3))\n",
    "        im = Image.fromarray(arr.astype('uint8'))\n",
    "        #image2=self.transform(image+torch.randn(tensor.size()) *std + mean2)\n",
    "        sample = {\n",
    "            \"image\": image.to(self.device),\n",
    "            \"noiseimage\":im.to(self.device)\n",
    "            #\"noiseimage\":image2.to(self.device),\n",
    "            #to do add noised image\n",
    "         #   \"label\": torch.from_numpy(label_onehot).to(self.device)\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d06b83a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n"
     ]
    }
   ],
   "source": [
    "label_dict = dict([(j,i) for (i,j) in sorted(list(enumerate(os.listdir('D:/OSU/Thesis/Fairness/MNIST/archive/trainingSet/trainingSet/'))))])\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ea0934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataloader(train_df, train_batch_size,device):\n",
    "    traindataset = CIFARDataSet(train_df, transform=transform.Compose([\n",
    "    transform.RandomCrop(32, padding=4),\n",
    "    transform.RandomHorizontalFlip(),\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]), device=device)\n",
    "    trainloader = DataLoader(traindataset, batch_size=train_batch_size,shuffle=True, num_workers=28)\n",
    "    return trainloader\n",
    "\n",
    "def get_test_dataloader(test_df,device):\n",
    "    valdataset = CIFARDataSet(test_df, transform=transform.Compose([\n",
    "        transform.ToTensor(),\n",
    "        transform.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]), device=device)\n",
    "    valloader = DataLoader(valdataset, batch_size=32, num_workers=28)\n",
    "    return valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc2057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:478: UserWarning: This DataLoader will create 28 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "iterations = 50\n",
    "number_of_epochs = 1\n",
    "learning_rate = 0.01\n",
    "pretrained = True\n",
    "i=4\n",
    "dataloader= get_train_dataloader(train_df, batch_size,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader= get_train_dataloader(train_df, batch_size,device)\n",
    "testloader = get_test_dataloader(test_df,device)\n",
    "\n",
    "optimzergen= torch.optim.SGD(generator.parameters(),lr=learning_rate)\n",
    "optimzerdis= torch.optim.SGD(generator.parameters(),lr=learning_rate)\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    noise=batch['noise']\n",
    "    image=batch['image']\n",
    "     \n",
    "    valid = Variable(Tensor(image.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "    fake = Variable(Tensor(noise.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "    trueimages = Variable(image.type(Tensor))\n",
    "    \n",
    "    #generator\n",
    "    optimzergen.zero_grad()\n",
    "    z = Variable(Tensor(np.random.normal(0, 1, (image.shape[0])))\n",
    "    y_pred=generator(z)             \n",
    "    g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "    g_loss.backward()\n",
    "    optimizergen.step()\n",
    "    \n",
    "                 \n",
    "    #Discrimintaor\n",
    "    optimzerdis.zero_grad()\n",
    "    real_loss = adversarial_loss(discriminator(trueimages), valid)\n",
    "    fake_loss = adversarial_loss(discriminator(y_pred.detach()), fake)\n",
    "    d_loss = (real_loss + fake_loss) / 2\n",
    "    d_loss.backward()\n",
    "    optimizerdis.step()\n",
    "\n",
    "    with open('loss.txt', 'w+') as f:\n",
    "                 for item in steady_latency:\n",
    "                       f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    print( \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n",
    "  \n",
    "        \n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
